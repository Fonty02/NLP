{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import gutenberg, PlaintextCorpusReader\n",
    "from nltk.util import ngrams\n",
    "from nltk.lm.preprocessing import pad_both_ends, padded_everygram_pipeline\n",
    "from nltk.lm import  MLE, Vocabulary\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = nltk.corpus.gutenberg.raw('chesterton-thursday.txt')\n",
    "sents = nltk.sent_tokenize(text) #divide text into sentences\n",
    "tokenized_text = [list(map(str.lower, word_tokenize(sent))) for sent in sents] #tokenize each sentence into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded sentence n=3:  ['<s>', '<s>', '[', 'the', 'man', 'who', 'was', 'thursday', 'by', 'g.', 'k.', 'chesterton', '1908', ']', 'to', 'edmund', 'clerihew', 'bentley', 'a', 'cloud', 'was', 'on', 'the', 'mind', 'of', 'men', ',', 'and', 'wailing', 'went', 'the', 'weather', ',', 'yea', ',', 'a', 'sick', 'cloud', 'upon', 'the', 'soul', 'when', 'we', 'were', 'boys', 'together', '.', '</s>', '</s>']\n",
      "Bigrams:  [('<s>', '['), ('[', 'the'), ('the', 'man'), ('man', 'who'), ('who', 'was'), ('was', 'thursday'), ('thursday', 'by'), ('by', 'g.'), ('g.', 'k.'), ('k.', 'chesterton'), ('chesterton', '1908'), ('1908', ']'), (']', 'to'), ('to', 'edmund'), ('edmund', 'clerihew'), ('clerihew', 'bentley'), ('bentley', 'a'), ('a', 'cloud'), ('cloud', 'was'), ('was', 'on'), ('on', 'the'), ('the', 'mind'), ('mind', 'of'), ('of', 'men'), ('men', ','), (',', 'and'), ('and', 'wailing'), ('wailing', 'went'), ('went', 'the'), ('the', 'weather'), ('weather', ','), (',', 'yea'), ('yea', ','), (',', 'a'), ('a', 'sick'), ('sick', 'cloud'), ('cloud', 'upon'), ('upon', 'the'), ('the', 'soul'), ('soul', 'when'), ('when', 'we'), ('we', 'were'), ('were', 'boys'), ('boys', 'together'), ('together', '.'), ('.', '</s>')]\n",
      "Trigrams:  [('<s>', '<s>', '['), ('<s>', '[', 'the'), ('[', 'the', 'man'), ('the', 'man', 'who'), ('man', 'who', 'was'), ('who', 'was', 'thursday'), ('was', 'thursday', 'by'), ('thursday', 'by', 'g.'), ('by', 'g.', 'k.'), ('g.', 'k.', 'chesterton'), ('k.', 'chesterton', '1908'), ('chesterton', '1908', ']'), ('1908', ']', 'to'), (']', 'to', 'edmund'), ('to', 'edmund', 'clerihew'), ('edmund', 'clerihew', 'bentley'), ('clerihew', 'bentley', 'a'), ('bentley', 'a', 'cloud'), ('a', 'cloud', 'was'), ('cloud', 'was', 'on'), ('was', 'on', 'the'), ('on', 'the', 'mind'), ('the', 'mind', 'of'), ('mind', 'of', 'men'), ('of', 'men', ','), ('men', ',', 'and'), (',', 'and', 'wailing'), ('and', 'wailing', 'went'), ('wailing', 'went', 'the'), ('went', 'the', 'weather'), ('the', 'weather', ','), ('weather', ',', 'yea'), (',', 'yea', ','), ('yea', ',', 'a'), (',', 'a', 'sick'), ('a', 'sick', 'cloud'), ('sick', 'cloud', 'upon'), ('cloud', 'upon', 'the'), ('upon', 'the', 'soul'), ('the', 'soul', 'when'), ('soul', 'when', 'we'), ('when', 'we', 'were'), ('we', 'were', 'boys'), ('were', 'boys', 'together'), ('boys', 'together', '.'), ('together', '.', '</s>'), ('.', '</s>', '</s>')]\n"
     ]
    }
   ],
   "source": [
    "padded_sent=list(pad_both_ends(tokenized_text[0], n=2))  #pad_both_ends adds padding to the beginning and end of the sentence\n",
    "bigrams=list(ngrams(padded_sent, n=2)) #ngrams creates bi-grams from the padded sentence\n",
    "padded_sent=list(pad_both_ends(tokenized_text[0], n=3)) #pad_both_ends adds padding to the beginning and end of the sentence. Qui mettiamo n=3 perch√® ci servono due parole precedenti\n",
    "trigrams=list(ngrams(padded_sent, n=3)) #ngrams creates tri-grams from the padded sentence\n",
    "print(\"Padded sentence n=3: \", padded_sent) #print padded sentence\n",
    "print(\"Bigrams: \", bigrams) #print bi-grams\n",
    "print(\"Trigrams: \", trigrams) #print tri-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training n-grams:  <generator object padded_everygram_pipeline.<locals>.<genexpr> at 0x0000022F305AF340>\n",
      "Padded sentences:  <itertools.chain object at 0x0000022F6175F640>\n"
     ]
    }
   ],
   "source": [
    "n = 2\n",
    "training_ngrams, padded_sents = padded_everygram_pipeline(n,tokenized_text)  #padded_everygram_pipeline creates a training set of n-grams and a list of padded sentences\n",
    "print(\"Training n-grams: \", training_ngrams) #print training n-grams\n",
    "print(\"Padded sentences: \", padded_sents) #print padded sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want a 3-gram model\n",
    "n = 3\n",
    "training_ngrams, padded_sents = padded_everygram_pipeline(n,\n",
    "tokenized_text)\n",
    "model = MLE(n)\n",
    "model.fit(training_ngrams, padded_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first two sententes of Caesar\n",
      "\n",
      "[The Tragedie of Julius Caesar by William Shakespeare 1599]\n",
      "\n",
      "\n",
      "Actus Primus. [The Tragedie of Julius Caesar by William Shakespeare 1599]\n",
      "\n",
      "\n",
      "Actus Primus.\n",
      "first two sententes of Hamlet\n",
      "\n",
      "[The Tragedie of Hamlet by William Shakespeare 1599]\n",
      "\n",
      "\n",
      "Actus Primus. [The Tragedie of Hamlet by William Shakespeare 1599]\n",
      "\n",
      "\n",
      "Actus Primus.\n",
      "first two sententes of Macbeth\n",
      "\n",
      "[The Tragedie of Macbeth by William Shakespeare 1603]\n",
      "\n",
      "\n",
      "Actus Primus. [The Tragedie of Macbeth by William Shakespeare 1603]\n",
      "\n",
      "\n",
      "Actus Primus.\n",
      "n. sententes in Caesar:  1592  sentences\n",
      "\n",
      "n. sententes in Hamlet:  2355  sentences\n",
      "\n",
      "n. sententes in Macbeth:  1465  sentences\n",
      "\n",
      "Total n. of sentences in our Shakespere corpus:  5412  sentences\n",
      "\n",
      "<Vocabulary with cutoff=1 unk_label='<UNK>' and 7806 items>\n",
      "*** Checking for some words in the vocabulary... ***\n",
      "\n",
      "brute\n",
      "lord\n",
      "thane\n",
      "('<UNK>', 'from', '<UNK>')\n",
      "1\n",
      "293\n",
      "25\n",
      "*** Now using the model... ***\n",
      "\n",
      "*** scores ***\n",
      "\n",
      "brute 9.46880030300161e-06\n",
      "lord 0.0027743584887794715\n",
      "thane 0.00023672000757504026\n",
      "aliens 0.0\n",
      "0.006825938566552901\n",
      "1.0\n",
      "0.02\n",
      "*** perplexity ***\n",
      "\n",
      "[('and', 'make', 'your')] 4.0\n",
      "[('into', 'the'), ('sea', 'and')] 5.281546822864043\n",
      "[('inter', 'will', 'win'), ('the', 'champions', 'league')] inf\n",
      "[('into', 'the', 'sky')] inf\n",
      "[('into', 'the', 'sea')] 19.000000000000007\n",
      "['by', 'your', 'pardon', 'sir', ',', 'he', 'is', ',', 'my', 'lord']\n",
      "['</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "['intended', 'towards', 'him', '?', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "['</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "['intended', 'towards', 'him', '?', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n"
     ]
    }
   ],
   "source": [
    "text1= nltk.corpus.gutenberg.raw('shakespeare-caesar.txt')\n",
    "text2= nltk.corpus.gutenberg.raw('shakespeare-hamlet.txt')\n",
    "text3=nltk.corpus.gutenberg.raw('shakespeare-macbeth.txt')\n",
    "\n",
    "sents1 = nltk.sent_tokenize(text1)\n",
    "sents2 = nltk.sent_tokenize(text2)\n",
    "sents3 = nltk.sent_tokenize(text3)\n",
    "print ('first two sententes of Caesar\\n')\n",
    "print  (sents1[0],sents1[0])\n",
    "print ('first two sententes of Hamlet\\n')\n",
    "print  (sents2[0],sents2[0])\n",
    "print ('first two sententes of Macbeth\\n')\n",
    "print  (sents3[0],sents3[0])\n",
    "sents=sents1+sents2+sents3\n",
    "print  ('n. sententes in Caesar: ',len(sents1),' sentences\\n')\n",
    "print  ('n. sententes in Hamlet: ',len(sents2),' sentences\\n')\n",
    "print  ('n. sententes in Macbeth: ',len(sents3),' sentences\\n')\n",
    "print  ('Total n. of sentences in our Shakespere corpus: ',len(sents),' sentences\\n')\n",
    "#sents.append(nltk.sent_tokenize(text2))\n",
    "#sents.append(nltk.sent_tokenize(text3))\n",
    "\n",
    "tokenized_text = [list(map(str.lower, word_tokenize(sent))) \n",
    "                  for sent in sents]\n",
    "\n",
    "\n",
    "\n",
    "# we want a 3-gram model\n",
    "n = 3\n",
    "training_ngrams, padded_sents = padded_everygram_pipeline(n, tokenized_text)\n",
    "model = MLE(n)\n",
    "model.fit(training_ngrams, padded_sents)\n",
    "print(model.vocab)\n",
    "print('*** Checking for some words in the vocabulary... ***\\n')\n",
    "print (model.vocab.lookup('brute'))\n",
    "print (model.vocab.lookup('lord'))\n",
    "print (model.vocab.lookup('thane'))\n",
    "print (model.vocab.lookup(['aliens', 'from', 'Mars']))\n",
    "print (model.counts['brute'])\n",
    "print (model.counts['lord'])\n",
    "print (model.counts['thane'])\n",
    "print('*** Now using the model... ***\\n')\n",
    "print('*** scores ***\\n')\n",
    "print ('brute', model.score('brute'))\n",
    "print ('lord', model.score('lord'))\n",
    "print ('thane', model.score('thane'))\n",
    "print ('aliens', model.score('aliens'))\n",
    "# P('is'|'lord')\n",
    "print (model.score('is', 'lord'.split()))\n",
    "print (model.score('of', 'the tragedie'.split()))\n",
    "# P('question'|'is the')\n",
    "print (model.score('question', 'is the'.split()))\n",
    "print('*** perplexity ***\\n')\n",
    "test1 = [('and', 'make', 'your')]\n",
    "test2 = [('into', 'the'), ('sea', 'and')]\n",
    "test3 = [('inter', 'will', 'win'), ('the', 'champions', 'league')]\n",
    "test4 = [('into', 'the', 'sky')]\n",
    "test5 = [('into', 'the', 'sea')]\n",
    "print (test1, model.perplexity(test1))\n",
    "print (test2, model.perplexity(test2))\n",
    "print (test3, model.perplexity(test3))\n",
    "print (test4, model.perplexity(test4))\n",
    "print (test5, model.perplexity(test5))\n",
    "print(model.generate(10))\n",
    "print(model.generate(10,random_seed=3))\n",
    "print(model.generate(10,random_seed=5))\n",
    "print(model.generate(10,random_seed=3))\n",
    "print(model.generate(10,random_seed=5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['that', 'they', 'follow', ',', 'did', 'loose', 'his', 'lustre', ':', 'i', 'will', 'not', 'come', 'to', 'my', 'father', 'much', 'offended', 'qu', '.']\n",
      "[':', 'tis', 'true', ',', 'octa', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "that they follow, did loose his lustre: i will not come to my father much offended qu.\n",
      ": tis true, octa.\n",
      "where we are, my lord bru.\n",
      "\n",
      "\n",
      "intended towards him?\n"
     ]
    }
   ],
   "source": [
    "text1= nltk.corpus.gutenberg.raw('shakespeare-caesar.txt')\n",
    "text2= nltk.corpus.gutenberg.raw('shakespeare-hamlet.txt')\n",
    "text3=nltk.corpus.gutenberg.raw('shakespeare-macbeth.txt')\n",
    "\n",
    "sents1 = nltk.sent_tokenize(text1)\n",
    "sents2 = nltk.sent_tokenize(text2)\n",
    "sents3 = nltk.sent_tokenize(text3)\n",
    "sents=sents1+sents2+sents3\n",
    "tokenized_text = [list(map(str.lower, word_tokenize(sent))) \n",
    "                  for sent in sents]\n",
    "\n",
    "\n",
    "\n",
    "# we want a 3-gram model\n",
    "n = 3\n",
    "training_ngrams, padded_sents = padded_everygram_pipeline(n, tokenized_text)\n",
    "model = MLE(n)\n",
    "model.fit(training_ngrams, padded_sents)\n",
    "print(model.generate(20, random_seed=0))\n",
    "print(model.generate(20, random_seed=1))\n",
    "\n",
    "detokenize = TreebankWordDetokenizer().detokenize\n",
    "\n",
    "def generate_sent(model, num_words, random_seed):\n",
    "    content = []\n",
    "    for token in model.generate(num_words, random_seed=random_seed):\n",
    "        if token == '<s>':\n",
    "            continue\n",
    "        if token == '</s>':\n",
    "            break\n",
    "        content.append(token)\n",
    "    return detokenize(content)\n",
    "\n",
    "\n",
    "print (generate_sent(model, 20, random_seed=0))\n",
    "print (generate_sent(model, 20, random_seed=1))\n",
    "print (generate_sent(model, 20, random_seed=2))\n",
    "print (generate_sent(model, 20, random_seed=3))\n",
    "print (generate_sent(model, 20, random_seed=4))\n",
    "print (generate_sent(model, 20, random_seed=5))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "picks it up! Democrats numbers are down big in new Quinnipiac poll just released . Wow . Unbelievable crowd\n",
      "17 other people!\n",
      "via my Facebook page in St. Joseph, Michigan . Streaming live - join us today because of my constant\n"
     ]
    }
   ],
   "source": [
    "detokenize = TreebankWordDetokenizer().detokenize\n",
    "def generate_sent(model, num_words, random_seed):\n",
    "    content = []\n",
    "    for token in model.generate(num_words, random_seed=random_seed):\n",
    "        if token == '<s>':\n",
    "            continue\n",
    "        if token == '</s>':\n",
    "            break\n",
    "        content.append(token)\n",
    "    return detokenize(content)  \n",
    "\n",
    "\n",
    "df = pd.read_csv('../Donald-Tweets!.csv')\n",
    "df.head()\n",
    "trump_corpus = list(df['Tweet_Text'].apply(word_tokenize))\n",
    "n = 3\n",
    "train_data, padded_sents = padded_everygram_pipeline(n, trump_corpus)\n",
    "trump_model = MLE(n)\n",
    "trump_model.fit(train_data, padded_sents)\n",
    "print(generate_sent(trump_model, num_words=20, random_seed=0))\n",
    "print(generate_sent(trump_model, num_words=20, random_seed=1))\n",
    "print(generate_sent(trump_model, num_words=20, random_seed=2))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../tweets_itapoltweets_1662817574_0'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     11\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tweetlist\n\u001b[32m     14\u001b[39m corpus_folder=\u001b[33m'\u001b[39m\u001b[33m../tweets_itapol\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m tweets = \u001b[43mread_Corpus\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus_folder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m (\u001b[33m'\u001b[39m\u001b[33mSome tweets:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m (tweets[\u001b[32m0\u001b[39m])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mread_Corpus\u001b[39m\u001b[34m(corpus_root)\u001b[39m\n\u001b[32m      3\u001b[39m tweetlist=[]\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m fileid \u001b[38;5;129;01min\u001b[39;00m tweetsfiles.fileids():\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcorpus_root\u001b[49m\u001b[43m+\u001b[49m\u001b[43mfileid\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m user_file:\n\u001b[32m      6\u001b[39m         parsed_json = json.load(user_file)\n\u001b[32m      7\u001b[39m         tweets=parsed_json.get(\u001b[33m'\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fonta\\Desktop\\Uni\\Repo\\NLP\\env\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:325\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    319\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    320\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    321\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    322\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    323\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '../tweets_itapoltweets_1662817574_0'"
     ]
    }
   ],
   "source": [
    "def read_Corpus(corpus_root):\n",
    "    tweetsfiles = PlaintextCorpusReader(corpus_root, '.*')\n",
    "    tweetlist=[]\n",
    "    for fileid in tweetsfiles.fileids():\n",
    "        with open(corpus_root+fileid) as user_file:\n",
    "            parsed_json = json.load(user_file)\n",
    "            tweets=parsed_json.get('data')\n",
    "            for tweet in tweets:\n",
    "                tweetlist.append(tweet.get('text'))\n",
    "                #print (tweet.get('text'))\n",
    "    return tweetlist\n",
    "\n",
    "\n",
    "corpus_folder='../tweets_itapol/'\n",
    "tweets = read_Corpus(corpus_folder)\n",
    "print ('Some tweets:\\n')\n",
    "print (tweets[0])\n",
    "print (tweets[1])\n",
    "print (tweets[2])\n",
    "\n",
    "tokenized_text = [list(map(str.lower, word_tokenize(tweet))) \n",
    "                  for tweet in tweets]\n",
    "\n",
    "# we want a 3-gram model\n",
    "n = 3\n",
    "training_ngrams, padded_sents = padded_everygram_pipeline(n, tokenized_text)\n",
    "model = MLE(n)\n",
    "model.fit(training_ngrams, padded_sents)\n",
    "\n",
    "detokenize = TreebankWordDetokenizer().detokenize\n",
    "\n",
    "def generate_sent(model, num_words, random_seed):\n",
    "    content = []\n",
    "    for token in model.generate(num_words, random_seed=random_seed):\n",
    "        if token == '<s>':\n",
    "            continue\n",
    "        if token == '</s>':\n",
    "            break\n",
    "        content.append(token)\n",
    "    return detokenize(content)\n",
    "\n",
    "print ('\\nGenerating some tweets with 3-grams...\\n')\n",
    "print ('\\n ***', generate_sent(model, 20, random_seed=10))\n",
    "print ('\\n ***',generate_sent(model, 20, random_seed=11))\n",
    "print ('\\n ***',generate_sent(model, 20, random_seed=12))\n",
    "print ('\\n ***',generate_sent(model, 20, random_seed=13))\n",
    "print ('\\n ***',generate_sent(model, 20, random_seed=14))\n",
    "print ('\\n ***',generate_sent(model, 20, random_seed=15))\n",
    "\n",
    "n = 4\n",
    "training_ngrams, padded_sents = padded_everygram_pipeline(n, tokenized_text)\n",
    "model = MLE(n)\n",
    "model.fit(training_ngrams, padded_sents)\n",
    "\n",
    "print ('\\nGenerating some tweets with 4-grams...\\n')\n",
    "print ('\\n ***', generate_sent(model, 20, random_seed=10))\n",
    "print ('\\n ***',generate_sent(model, 20, random_seed=11))\n",
    "print ('\\n ***',generate_sent(model, 20, random_seed=12))\n",
    "print ('\\n ***',generate_sent(model, 20, random_seed=13))\n",
    "print ('\\n ***',generate_sent(model, 20, random_seed=14))\n",
    "print ('\\n ***',generate_sent(model, 20, random_seed=15))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
