{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvrJarQK5xcH",
        "outputId": "f2eee1af-94a9-4219-9bd6-367b77a0b429"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spaCy in /usr/local/lib/python3.11/dist-packages (3.8.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spaCy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spaCy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spaCy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spaCy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spaCy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spaCy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spaCy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spaCy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spaCy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spaCy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spaCy) (0.15.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spaCy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spaCy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spaCy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spaCy) (2.11.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spaCy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spaCy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spaCy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spaCy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spaCy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spaCy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spaCy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spaCy) (4.13.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spaCy) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spaCy) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spaCy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spaCy) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spaCy) (2025.4.26)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spaCy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spaCy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spaCy) (8.2.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spaCy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spaCy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spaCy) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spaCy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spaCy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spaCy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spaCy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spaCy) (2.19.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spaCy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spaCy) (0.1.2)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.11/dist-packages (3.8.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from keras) (2.0.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras) (0.0.9)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras) (3.13.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras) (0.15.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras) (0.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from optree->keras) (4.13.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
            "Collecting it-core-news-lg==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/it_core_news_lg-3.8.0/it_core_news_lg-3.8.0-py3-none-any.whl (567.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m567.9/567.9 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: it-core-news-lg\n",
            "Successfully installed it-core-news-lg-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('it_core_news_lg')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!pip install -U spaCy\n",
        "!pip install keras\n",
        "!python -m spacy download it_core_news_lg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCJOwDePW6zd"
      },
      "source": [
        "# Text categorization using Deep Learning (with Keras)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBXRdNTOXL1Q"
      },
      "source": [
        "Now you are finally ready to experiment with Deep Learning and Keras. Keras supports two main types of models. You have the Sequential model API which you are going to see in use in this tutorial and the functional API which can do everything of the Sequential model but it can be also used for advanced models with complex network architectures.\n",
        "\n",
        "The Sequential model is a linear stack of layers, where you can use the large variety of available layers in Keras. The most common layer is the Dense layer which is your regular densely connected neural network layer.\n",
        "\n",
        "We need to prepare training and testing data.\n",
        "\n",
        "Data available here: https://drive.google.com/drive/folders/1pQKHrUth2x3lR-W74LKwtke-kQRIbR6U?usp=drive_link"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZy-nxEFYLIL",
        "outputId": "514ca785-bba1-48a9-8c74-80e8426772fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of features:  19326\n",
            "Number of classes:  14\n"
          ]
        }
      ],
      "source": [
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import json\n",
        "\n",
        "df = open('dataset_05.json','r')\n",
        "X_text = []\n",
        "y = []\n",
        "for line in df:\n",
        "  j = json.loads(line)\n",
        "  #text = j['title']\n",
        "  text = j['title']+' '+j['desc']\n",
        "  #text = j['title']+' '+j['desc']+' '+(j['text'] if 'text' in j  else '')\n",
        "  X_text.append(text)\n",
        "  y.append(j['topic']) # the labels\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "vectorizer.fit(X_text)\n",
        "X = vectorizer.transform(X_text)\n",
        "\n",
        "X_f, X_test, y_f, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_f, y_f, test_size=0.1, random_state=42)\n",
        "\n",
        "# Number of features\n",
        "input_dim = X_train.shape[1]\n",
        "print(\"Number of features: \",input_dim)\n",
        "\n",
        "# binarize labels\n",
        "encoder = LabelBinarizer()\n",
        "y_train = encoder.fit_transform(y_train)\n",
        "y_val = encoder.transform(y_val)\n",
        "y_test = encoder.transform(y_test)\n",
        "nc = encoder.classes_.size\n",
        "print(\"Number of classes: \",nc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgaJQsge7XS7"
      },
      "source": [
        "We design the network and compile it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "qgHFXwyw7YjW",
        "outputId": "b6ce9a96-242e-4b65-ae95-a4929f6fe6fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │     \u001b[38;5;34m9,895,424\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m)             │         \u001b[38;5;34m3,598\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">9,895,424</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,598</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m10,030,350\u001b[0m (38.26 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,030,350</span> (38.26 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m10,030,350\u001b[0m (38.26 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,030,350</span> (38.26 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "model = Sequential()\n",
        "model.add(layers.Dense(512, input_dim=input_dim, activation='relu'))\n",
        "model.add(layers.Dense(256, activation='relu'))\n",
        "model.add(layers.Dense(nc, activation='softmax')) # nc is the number of classes\n",
        "\n",
        "# compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9sQhvbN75RJ"
      },
      "source": [
        "Now, we can fit the network on training data. We use testing data for validation.\n",
        "The fit function requires the number of epochs and the batch size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUxqZxLKZXzx",
        "outputId": "63081b7a-d5a9-4e72-dc20-ec5720199e99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 224ms/step - accuracy: 0.4980 - loss: 1.7575 - val_accuracy: 0.8106 - val_loss: 0.6199\n",
            "Epoch 2/5\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 229ms/step - accuracy: 0.9657 - loss: 0.1379 - val_accuracy: 0.8106 - val_loss: 0.6368\n",
            "Epoch 3/5\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 223ms/step - accuracy: 0.9983 - loss: 0.0126 - val_accuracy: 0.8119 - val_loss: 0.6621\n",
            "Epoch 4/5\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 226ms/step - accuracy: 0.9992 - loss: 0.0033 - val_accuracy: 0.8068 - val_loss: 0.6662\n",
            "Epoch 5/5\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 232ms/step - accuracy: 0.9992 - loss: 0.0036 - val_accuracy: 0.8106 - val_loss: 0.6794\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x78e6bc6c8f10>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "from keras.backend import clear_session\n",
        "clear_session() # Make sure to call clear_session() before you start training the model again\n",
        "\n",
        "model.fit(X_train, y_train, epochs=5, verbose=True, validation_data=(X_val, y_val), batch_size=64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5w8j-1PniVij"
      },
      "source": [
        "Compute accuracy on both training and test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4Avjf7SiYzA",
        "outputId": "54b16360-0200-4ec6-90a8-fdad5bed79d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy: 0.9997\n",
            "Testing Accuracy:  0.8156\n"
          ]
        }
      ],
      "source": [
        "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOU3B_UpmCRu"
      },
      "source": [
        "# Using word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKL1v_mKmE3J"
      },
      "source": [
        "We can directly use pretrained word embeddings in our model as input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "rHCE_H309TLq"
      },
      "outputs": [],
      "source": [
        "df = open('dataset_05.json','r')\n",
        "X = []\n",
        "y = []\n",
        "for line in df:\n",
        "  j = json.loads(line)\n",
        "  #text = j['title']\n",
        "  text = j['title']+' '+j['desc']\n",
        "  #text = j['title']+' '+(j['text'] if 'text' in j  else '')\n",
        "  X.append(text.lower())\n",
        "  y.append(j['topic']) # the labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4puk0ik9Z6N"
      },
      "source": [
        "Now we need to tokenize the data into a format that can be used by word embeddings. Keras offers a couple of convenience methods for text preprocessing and sequence preprocessing which you can employ to prepare your text.\n",
        "\n",
        "You can start by using the Tokenizer utility class which can vectorize a text corpus into a list of integers. Each integer maps to a value in a dictionary that encodes the entire corpus, with the keys in the dictionary being the vocabulary terms themselves. You can add the parameter num_words, which is responsible for setting the size of the vocabulary. The most common num_words words will be then kept."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-preprocessing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bxZFbAVINGTR",
        "outputId": "abe221af-8315-41c3-ec2b-16e61927784f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-preprocessing\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.11/dist-packages (from keras-preprocessing) (2.0.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from keras-preprocessing) (1.17.0)\n",
            "Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: keras-preprocessing\n",
            "Successfully installed keras-preprocessing-1.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvXz_mlJ9aXq",
        "outputId": "93201d76-589b-40ce-bda7-bc1ab7175489"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 21785\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X)\n",
        "\n",
        "X_f, X_test, y_f, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_f, y_f, test_size=0.1, random_state=42)\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)\n",
        "X_val = tokenizer.texts_to_sequences(X_val)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Vocabulary size:',vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nn82oMzu9eCl"
      },
      "source": [
        "One problem that we have is that each text sequence has in most cases different length of words. To counter this, you can use pad_sequence() which simply pads the sequence of words with zeros. By default, it prepends zeros but we want to append them. Typically it does not matter whether you prepend or append zeros.\n",
        "\n",
        "Additionally you would want to add a maxlen parameter to specify how long the sequences should be. This cuts sequences that exceed that number. In the following code, you can see how to pad sequences with Keras:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNh23BuJ9uT9",
        "outputId": "0d60ba26-a43c-4d83-f78d-a6959de5f85b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 267   28   12 1714  264 6645  936 6646 1464 6647   33   11   84  267\n",
            "   28   12 1714  264 6645  936 6646 1464 6647   33   11   84    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0]\n"
          ]
        }
      ],
      "source": [
        "from keras.utils import pad_sequences\n",
        "\n",
        "maxlen = 128\n",
        "#Per le reti che gestiscono sequenze ovviamente le sequenze devono avere tutte la stessa sequenza. Per evitare problemi dunque si mette una lunghezza massima. Per quelle piu corte si aggiunge il padding alla fine. Quelle piu corte si tronca (ovviamente meglio evitare di troncare)\n",
        "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
        "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
        "X_val = pad_sequences(X_val, padding='post', maxlen=maxlen)\n",
        "\n",
        "print(X_train[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpHd8tSb9u4D"
      },
      "source": [
        "We need to binarize the labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZqpFgaf9eZg",
        "outputId": "2cd365e4-5f8f-4a8d-93a3-144a8927dabf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "salute\n",
            "[0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
            "Number of classes: 14\n"
          ]
        }
      ],
      "source": [
        "# binarize labels\n",
        "encoder = LabelBinarizer()\n",
        "print(y_train[0])\n",
        "y_train = encoder.fit_transform(y_train)\n",
        "print(y_train[0])\n",
        "y_test = encoder.transform(y_test)\n",
        "y_val = encoder.transform(y_val)\n",
        "nc = encoder.classes_.size\n",
        "print('Number of classes:',nc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZbPqL8b9jU2"
      },
      "source": [
        "You can see in the next example how you can load the embedding matrix. Each line in the file starts with the word and is followed by the embedding vector for the particular word.\n",
        "\n",
        "This is a large file with each line representing a word followed by its vector as a stream of floats.\n",
        "\n",
        "Since you don’t need all words, you can focus on only the words that we have in our vocabulary. Since we have only a limited number of words in our vocabulary.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wc5e1GgNN-dz"
      },
      "source": [
        "Download pre-trained word embeddings for Italian from fasttext."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRDFPdgaN9sh",
        "outputId": "2912e555-578b-4a01-a7ac-08682c736388"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-16 07:02:08--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.it.300.vec.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 18.173.166.74, 18.173.166.51, 18.173.166.31, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|18.173.166.74|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1272825284 (1.2G) [binary/octet-stream]\n",
            "Saving to: ‘cc.it.300.vec.gz’\n",
            "\n",
            "cc.it.300.vec.gz    100%[===================>]   1.18G   131MB/s    in 14s     \n",
            "\n",
            "2025-05-16 07:02:22 (86.9 MB/s) - ‘cc.it.300.vec.gz’ saved [1272825284/1272825284]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget 'https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.it.300.vec.gz'\n",
        "!gunzip cc.it.300.vec.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDEUtjJ371GR",
        "outputId": "27365213-3e0d-41c0-8fb7-2bd84f7f96df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2000000 300\n",
            ", -0.0624 -0.0432 -0.3535 -0.0145 0.0690 0.0831 0.0784 0.0153 0.4491 0.1494 0.0392 0.0331 -0.0138 -0.0321 0.0813 0.0449 0.0506 -0.0302 -0.0460 -0.0900 0.0872 -0.0460 -0.0014 -0.0633 -0.0683 -0.0064 -0.0802 0.0366 -0.0948 0.0211 -0.0140 0.0504 -0.0243 -0.0205 -0.0424 -0.0105 -0.0013 -0.0270 0.0189 0.1892 0.0491 -0.0239 -0.0399 -0.0001 -0.0192 0.1326 0.0995 -0.0239 0.0485 0.1064 -0.0603 0.0197 -0.0582 -0.0168 0.0471 -0.0094 -0.0000 -0.0562 0.0642 0.0338 -0.0096 -0.0799 0.0620 -0.0072 -0.0635 -0.0803 0.0618 -0.0305 -0.0152 -0.0265 0.0226 -0.0361 0.0489 0.0985 0.1611 0.0050 0.1271 0.2563 -0.0871 0.0338 0.0617 0.0266 -0.0647 0.0704 -0.1108 -0.0088 0.0403 -0.0116 0.0528 0.0304 0.0558 0.0045 -0.0231 -0.1034 -0.6818 -0.2181 0.0567 0.0305 0.0937 -0.0283 -0.0449 -0.0081 -0.0211 0.0494 0.0552 0.1646 0.0341 -0.0076 0.0265 -0.0632 -0.1509 0.0787 0.2853 -0.0154 -0.0769 0.0390 -0.0053 0.0110 0.0239 0.0178 0.0164 -0.0123 -0.0115 -0.0396 -0.0072 -0.0153 -0.0343 -0.0875 0.0568 1.2602 -0.0168 0.0166 0.0556 -0.0371 -0.0669 -0.0078 -0.0268 -0.0499 0.0758 -0.0577 -0.0371 0.0371 -0.0471 -0.0389 0.0341 -0.0565 -0.0135 -0.0516 0.0088 -0.0891 -0.0172 0.0416 -0.0535 -0.1343 0.0239 -0.0604 0.0425 -0.0272 0.1557 0.0366 -0.0407 0.0361 -0.0728 -0.0776 -0.1126 -0.0455 -0.0333 0.0413 -0.0055 0.0248 -0.0192 -0.0201 0.0957 0.0032 0.0650 -0.0194 0.0088 -0.2575 -0.0661 0.0273 -0.0376 0.0293 -0.0428 -0.0482 0.0417 -0.0565 -0.0293 -0.0493 -0.0007 0.0700 -0.0041 0.0434 0.0282 -0.1361 0.0718 0.0016 0.0313 0.0413 -0.3153 -0.0015 -0.0299 -0.0801 -0.0262 -0.0252 0.0262 -0.1131 -0.1549 -0.0370 0.0678 -0.1534 -0.0282 -0.0112 -0.0809 -0.0124 0.0335 -0.0171 -0.1018 -0.0027 -0.1000 0.0181 0.0727 -0.0410 0.0439 -0.0180 -0.0128 -0.0282 0.0008 0.2352 0.1079 0.0894 0.0202 -0.0642 0.0458 -0.0399 0.0028 -0.0189 -0.0081 -0.0002 -0.0200 -0.0504 0.1013 -0.0707 0.0589 0.0230 -0.0747 -0.0605 0.0003 -0.0909 0.0643 0.1492 -0.0479 0.0032 -0.0795 0.0665 -0.0194 -0.0504 0.0619 -0.0238 -0.1555 -0.0508 -0.0195 0.0066 0.0063 0.2250 -0.0962 0.0076 0.0368 0.0004 -0.0087 -0.0176 -0.0443 0.0025 0.0671 -0.2008 -0.0866 0.0075 -0.3059 -0.0043 0.1116 0.0746 0.0274 -0.0406 -0.0397 0.0327 0.1098 0.0896 0.1746 -0.0660 0.0063 -0.0002 0.0138 -0.1152 -0.0835 -0.0080 -0.1408 0.0451 0.0201 0.0918 -0.0478 0.0164\n",
            "di -0.0209 -0.0453 0.2433 0.0213 0.0544 0.0339 0.0099 -0.0682 -0.2366 -0.0152 0.0551 -0.0340 -0.1774 -0.0039 -0.0196 -0.1140 0.0611 -0.1588 0.0103 -0.0370 -0.0821 0.0247 0.0528 -0.0239 -0.0051 -0.4049 -0.0248 -0.0207 -0.0100 0.2256 0.1181 -0.1149 0.0336 -0.0629 -0.0202 0.0542 -0.0111 -0.0025 0.0285 0.1105 -0.0285 0.0037 0.0770 -0.0811 0.1433 -0.1567 -0.0258 -0.0159 -0.1026 0.1190 0.1163 -0.1211 -0.0077 -0.0010 -0.0576 0.0285 0.2326 -0.1005 -0.0203 -0.0359 0.0258 0.0640 -0.0593 0.0074 0.0099 0.0826 0.0148 -0.0313 -0.0349 -0.1369 -0.0133 0.0826 0.0380 -0.0734 0.0469 -0.0139 -0.0871 -0.0110 -0.0158 0.0202 0.0054 0.4215 -0.0277 0.1584 0.2401 0.1650 0.0646 -0.0358 0.0448 -0.1233 0.0360 0.0132 0.0406 -0.0462 -0.6705 0.1065 -0.0632 0.0075 0.1874 0.0657 -0.4269 -0.0979 -0.0162 -0.0023 0.0696 0.0631 -0.0913 -0.3772 -0.0940 -0.3831 -0.0794 -0.1240 -0.4585 -0.0002 0.0674 0.0334 -0.0336 0.0465 0.0506 -0.2053 0.0469 0.0060 0.0303 -0.0936 -0.1164 0.1240 0.1153 -0.0184 0.0596 1.6247 0.0127 0.1841 0.1061 0.0005 -0.0020 -0.0221 -0.0390 -0.0019 -0.5927 -0.0248 0.0407 -0.0231 -0.0754 -0.0356 0.0513 -0.0526 0.0600 -0.0287 0.0246 -0.0818 0.0426 0.0203 -0.0576 0.0927 0.0145 0.0884 -0.0790 -0.0549 0.0688 -0.0110 0.0200 0.0183 -0.0198 -0.0337 0.0801 -0.0762 0.0115 -0.0042 -0.0285 0.0925 -0.4038 -0.0015 0.3036 -0.0025 0.0751 -0.1259 0.0216 -0.1621 0.0052 -0.0469 -0.0046 -0.0246 -0.0196 -0.0064 0.0537 -0.1379 0.1248 0.0015 -0.1971 -0.0203 0.0557 -0.0766 0.0808 0.3282 -0.0373 0.0092 -0.3345 -0.0007 -0.1953 -0.0415 -0.0492 -0.1065 0.0027 0.0500 0.0628 -0.0845 -0.1627 0.0697 -0.0047 0.2017 0.1059 0.0256 0.0679 0.0350 0.0942 0.1074 0.1581 -0.0660 -0.1784 -0.0034 0.0086 0.1304 0.0529 0.0838 0.0518 -0.0242 0.0902 0.2896 0.0469 0.1402 0.0436 0.1583 0.0698 0.0121 -0.0036 -0.0360 -0.0298 0.0529 -0.2458 -0.0908 -0.1615 0.0025 -0.0220 -0.2850 -0.0008 0.0281 -0.0387 -0.1546 0.0382 0.2163 0.0094 -0.0117 -0.2869 0.0763 -0.0754 0.0602 0.0396 -0.0284 -0.0068 -0.2030 0.0736 0.0924 0.4004 -0.0937 -0.0012 0.0751 0.0775 -0.0681 0.0307 -0.0238 -0.2441 0.0681 -0.1686 0.0362 -0.1194 -0.0189 -0.2712 0.0250 0.1802 -0.0056 0.2315 -0.0243 0.0385 0.0445 0.0307 0.0415 0.0906 0.0794 0.0394 0.0034 -0.0091 -0.0085 0.0590 0.0646 -0.2023 0.0277 0.0111 -0.0775 0.0412 0.1437\n",
            ". -0.0646 -0.0110 -0.4754 0.0389 0.0226 -0.1143 -0.1474 -0.0213 0.6390 -0.1661 0.0718 0.0066 0.1191 -0.0401 0.0539 0.0257 0.0458 0.1118 -0.0058 -0.0545 0.0348 0.0606 0.0335 0.0305 -0.0238 0.0664 0.0811 -0.0029 0.0584 -0.0823 0.0588 0.3111 0.0054 0.0405 -0.0147 -0.0234 -0.0605 0.0424 0.0078 -0.0400 0.0326 -0.0468 0.1008 -0.0306 -0.0047 -0.0738 -0.0984 -0.0277 0.1025 0.1976 0.0227 0.0180 -0.0315 0.0407 -0.0327 -0.0355 -0.1466 -0.0276 0.1924 -0.0279 0.1153 0.2225 0.0880 -0.1359 0.0190 0.0063 0.0198 -0.0335 0.0629 -0.0726 -0.0351 -0.0415 0.0240 0.0127 -0.0105 -0.0853 0.3631 -0.3852 0.0455 0.1032 0.0715 0.2927 0.0470 0.2260 -0.0442 -0.0461 0.0134 0.0188 0.0687 -0.0812 -0.0049 0.0177 0.0545 -0.1206 -0.8936 0.1761 0.0255 0.0188 0.3846 -0.0573 0.0749 -0.0106 -0.0080 0.0913 0.0717 0.0449 0.0417 -0.0073 -0.1138 0.1355 -0.1645 -0.0098 -0.0904 0.0018 0.0334 0.0465 -0.0256 0.0668 0.0452 0.2075 0.0914 -0.0686 -0.0496 -0.0961 -0.0555 0.0545 0.1407 -0.0797 0.0722 1.0896 -0.1388 0.1694 -0.2872 -0.0131 -0.0372 -0.0214 0.0006 -0.0443 -0.0498 -0.0230 0.0633 -0.0328 -0.0368 -0.0581 0.0505 -0.0248 -0.0732 -0.1223 -0.0045 -0.1978 0.0107 -0.0162 0.0276 -0.1572 0.0181 -0.0589 0.1550 0.3359 0.2083 0.0751 -0.0594 -0.0546 0.0201 0.0404 0.0100 -0.1224 -0.0347 -0.0658 0.0201 -0.0235 -0.0127 0.0545 -0.6591 0.1212 -0.0330 -0.0291 0.0218 -0.3676 0.0522 -0.0609 -0.0076 0.0498 0.0599 -0.0644 0.0256 0.0347 0.0449 -0.0360 0.4240 -0.1217 -0.0078 -0.0114 -0.0203 -0.2942 0.0571 -0.0540 0.3911 -0.0549 -0.3936 -0.0562 -0.1121 0.0092 -0.0058 0.0579 0.0315 -0.1574 -0.0478 0.0078 0.0040 0.0826 -0.0135 -0.0406 0.0508 -0.0583 0.1489 0.0672 0.0453 -0.0361 -0.2766 -0.0866 -0.0772 -0.2421 -0.0981 0.0552 0.1302 -0.0359 -0.0310 -0.1581 -0.0435 0.0300 0.0086 -0.0965 0.2552 0.0532 -0.0075 0.0186 -0.0320 0.0392 0.0256 -0.1027 0.0061 0.1309 -0.1030 0.0461 -0.0013 0.0175 -0.1549 -0.0154 0.1108 0.0781 -0.1013 -0.0029 0.2127 -0.0211 0.0954 0.0647 0.0735 0.0819 0.1578 -0.0102 0.0266 0.0148 0.2118 -0.1158 0.0024 -0.0055 0.0222 -0.0654 0.0406 -0.0225 0.0017 -0.0225 -0.1711 0.1314 -0.0101 0.0530 -0.3806 0.1056 0.2273 0.0214 0.0188 -0.1012 0.0284 0.0180 0.0305 -0.0308 0.1442 0.0202 -0.0051 -0.0506 -0.0482 -0.1066 -0.0893 0.1085 -0.2742 0.0189 -0.1089 0.0257 -0.0613 -0.0934\n",
            "</s> 0.0372 0.1142 -0.4738 0.0723 0.0050 0.0120 -0.0768 -0.0571 0.8470 0.0152 -0.0130 -0.0200 0.0253 -0.0614 -0.0496 0.0530 0.0033 0.0224 0.0175 -0.0122 0.0071 0.2288 -0.0170 0.0188 0.0083 -0.1121 0.0652 -0.0379 0.1247 0.0866 0.0459 0.1222 -0.0609 -0.0315 -0.0391 -0.1150 0.0789 0.0813 0.0637 -0.0243 0.0801 0.0308 0.0474 0.0309 0.0295 0.0303 0.0050 0.0082 -0.0234 -0.0564 -0.0608 0.2121 -0.0753 -0.0357 -0.0064 0.0234 -0.2781 0.0176 0.2228 -0.0478 0.0064 -0.0321 0.3887 -0.0610 -0.0189 0.0408 -0.0327 0.0151 -0.1339 0.0485 0.0004 0.0500 -0.0170 0.1900 -0.0348 -0.0215 0.4799 -0.2202 -0.0537 0.0473 0.0353 0.0476 -0.1099 0.2586 -0.0674 -0.2194 -0.0678 0.0297 0.0017 -0.2291 0.0321 0.0091 0.0210 0.0401 -1.2350 -0.2676 -0.0096 0.0304 0.2970 0.0185 -0.0060 0.0014 -0.0782 0.0052 0.0089 0.3354 -0.0030 0.1548 -0.0610 0.1626 -0.0160 -0.1104 -0.1189 0.0012 0.0257 0.0112 0.0612 0.0443 0.1369 0.1134 0.0142 -0.0554 -0.0936 -0.0447 -0.0275 -0.0004 0.1581 0.0163 -0.0395 1.6338 -0.0173 0.0051 -0.3363 -0.0296 -0.0433 0.0203 -0.0374 -0.0025 -0.2090 -0.0802 0.1370 -0.0084 -0.1113 -0.0031 -0.0132 0.0669 0.0410 -0.0493 0.0522 -0.2082 -0.0501 -0.0151 -0.1171 -0.1103 0.2236 -0.0337 0.0072 0.1484 0.2405 0.0220 -0.1323 -0.0796 -0.0357 0.0627 -0.0981 -0.0222 -0.0336 0.0494 -0.0303 0.0322 0.2546 -0.0151 -0.1753 -0.0389 0.0093 0.0641 0.0141 -0.4688 0.0658 -0.0022 -0.0497 -0.0331 0.0640 0.0353 0.1352 -0.1303 0.0088 0.0815 0.4043 0.0094 0.0163 0.0251 0.2590 -0.1748 0.0119 -0.0321 0.4599 -0.0617 -0.3176 -0.0147 -0.0279 0.0004 -0.0229 -0.0346 -0.0192 0.0245 -0.2875 0.0426 0.0203 0.3441 -0.0344 -0.0902 0.0265 0.0078 0.0272 -0.0443 -0.0054 -0.1190 -0.1198 -0.0408 0.0302 -0.0874 -0.0900 0.0467 0.1213 -0.0254 0.0562 0.2430 -0.0121 0.0313 0.0108 -0.4985 0.0401 0.0059 0.1081 0.0255 -0.0085 -0.0963 0.2029 0.0364 -0.0264 -0.0485 -0.0664 0.2645 0.0081 0.1659 -0.1489 -0.4770 0.0244 0.2293 -0.0068 0.0567 -0.2023 0.0227 0.0057 0.0306 0.0220 0.0032 0.1478 -0.1260 -0.0862 -0.0132 0.3039 0.2329 0.1849 0.0285 -0.0271 -0.0775 0.0347 -0.0014 -0.0069 -0.0361 -0.2386 0.1034 0.1906 0.0289 -0.6337 0.1771 1.5811 -0.0384 -0.2083 -0.0350 -0.0074 0.0615 0.0809 -0.1024 0.3195 0.0247 0.0232 -0.0059 -0.0003 0.0290 0.0392 0.0455 -0.3435 -0.0230 -0.0141 -0.0257 0.0174 -0.0001\n"
          ]
        }
      ],
      "source": [
        "!head -n 5 cc.it.300.vec\n",
        "\n",
        "#2M di embedding lunghi 300\n",
        "\n",
        "#per ogni riga abbiamo carattere [embedding]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "moeHzDiVmGHp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "#prendiamo solo gli embedding che stanno nel nostro vocabolario, non tutti\n",
        "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
        "    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "\n",
        "    with open(filepath) as f:\n",
        "        f.readline # skip first line that contains word space info\n",
        "        for line in f:\n",
        "            word, *vector = line.split()\n",
        "            if word in word_index: # load only word embeddings into the vocabulary\n",
        "                idx = word_index[word]\n",
        "                embedding_matrix[idx] = np.array(\n",
        "                    vector, dtype=np.float32)[:embedding_dim]\n",
        "    return embedding_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c2_exiQmmKy"
      },
      "source": [
        "You can use this function now to retrieve the embedding matrix.\n",
        "We use word embeddings from fasttext."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "GFC0-HHHn2BB"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 300\n",
        "embedding_matrix = create_embedding_matrix('cc.it.300.vec', tokenizer.word_index, embedding_dim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tw0wGsbKHQIZ"
      },
      "source": [
        "Vocabulary coverage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAXVLe0VHQ41",
        "outputId": "1fe412be-79ec-4d5f-e283-9d80fda9d883"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary coverage:  0.7539132430571495\n"
          ]
        }
      ],
      "source": [
        "nonzero_elements = np.count_nonzero(np.count_nonzero(embedding_matrix, axis=1))\n",
        "print('Vocabulary coverage: ',nonzero_elements / vocab_size)\n",
        "\n",
        "#vediamo quante valore del training hanno un embedding\n",
        "#le altre verranno inizializzate con embedding a 0  (oppure Random, oppure centroide degli altri embedding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KK9Kr9u9zv_"
      },
      "source": [
        "We now design the new model with the word embeddings and the GlobalMaxPool layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "id": "5s7NYn68nvGs",
        "outputId": "ae6a16b9-7fc4-4281-ca89-ce1b7b37f7b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │     \u001b[38;5;34m6,535,500\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling1d        │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">6,535,500</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling1d        │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m6,535,500\u001b[0m (24.93 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,535,500</span> (24.93 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m6,535,500\u001b[0m (24.93 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,535,500</span> (24.93 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "model = Sequential()\n",
        "model.add(layers.Embedding(vocab_size, embedding_dim,\n",
        "                           weights=[embedding_matrix],\n",
        "                           input_length=maxlen,\n",
        "                           trainable=False,))\n",
        "model.add(layers.GlobalAveragePooling1D())\n",
        "model.add(layers.Dense(512, activation='relu'))\n",
        "model.add(layers.Dense(nc, activation='softmax'))\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "\n",
        "#rispetto a prima ora abbiamo gli embedding. Quindi inseriamo un layer con tutte le caratteristiche dei nostri embedding\n",
        "\n",
        "#nel layer dense non posso far entrare tanti vettori, ma uno solo. Si utilizza il Pooling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3Vg-WHxMsZu"
      },
      "source": [
        "Fit the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPFscDAWoL-r",
        "outputId": "ffa51e96-b520-4ba4-ead0-f708651cb68b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.2491 - loss: 2.5123 - val_accuracy: 0.2879 - val_loss: 2.3357\n",
            "Epoch 2/50\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.3140 - loss: 2.2393 - val_accuracy: 0.3182 - val_loss: 2.1003\n",
            "Epoch 3/50\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.3658 - loss: 2.0200 - val_accuracy: 0.4268 - val_loss: 1.8893\n",
            "Epoch 4/50\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.4438 - loss: 1.8311 - val_accuracy: 0.4684 - val_loss: 1.7340\n",
            "Epoch 5/50\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.4961 - loss: 1.6681 - val_accuracy: 0.5290 - val_loss: 1.6023\n",
            "Epoch 6/50\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5336 - loss: 1.5661 - val_accuracy: 0.5391 - val_loss: 1.5057\n",
            "Epoch 7/50\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.5627 - loss: 1.4660 - val_accuracy: 0.5682 - val_loss: 1.4324\n",
            "Epoch 8/50\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 28ms/step - accuracy: 0.5862 - loss: 1.3826 - val_accuracy: 0.5795 - val_loss: 1.3672\n",
            "Epoch 9/50\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.6084 - loss: 1.3225 - val_accuracy: 0.5960 - val_loss: 1.3133\n",
            "Epoch 10/50\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6302 - loss: 1.2448 - val_accuracy: 0.6212 - val_loss: 1.2714\n",
            "Epoch 11/50\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.6345 - loss: 1.2221 - val_accuracy: 0.6250 - val_loss: 1.2326\n",
            "Epoch 12/50\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.6531 - loss: 1.1754 - val_accuracy: 0.6288 - val_loss: 1.2061\n",
            "Epoch 13/50\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.6528 - loss: 1.1373 - val_accuracy: 0.6402 - val_loss: 1.1760\n",
            "Epoch 14/50\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.6658 - loss: 1.1135 - val_accuracy: 0.6414 - val_loss: 1.1621\n",
            "Epoch 15/50\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.6755 - loss: 1.0788 - val_accuracy: 0.6528 - val_loss: 1.1424\n",
            "Epoch 16/50\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.6951 - loss: 1.0195 - val_accuracy: 0.6553 - val_loss: 1.1235\n",
            "Epoch 17/50\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - accuracy: 0.6892 - loss: 1.0196 - val_accuracy: 0.6641 - val_loss: 1.1044\n",
            "Epoch 18/50\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.7034 - loss: 0.9918 - val_accuracy: 0.6641 - val_loss: 1.0890\n",
            "Epoch 19/50\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7039 - loss: 0.9749 - val_accuracy: 0.6679 - val_loss: 1.0810\n",
            "Epoch 20/50\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7115 - loss: 0.9581 - val_accuracy: 0.6654 - val_loss: 1.0709\n",
            "Epoch 21/50\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7166 - loss: 0.9453 - val_accuracy: 0.6818 - val_loss: 1.0604\n",
            "Epoch 22/50\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.7162 - loss: 0.9321 - val_accuracy: 0.6793 - val_loss: 1.0492\n",
            "Epoch 23/50\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - accuracy: 0.7311 - loss: 0.9079 - val_accuracy: 0.6742 - val_loss: 1.0493\n",
            "Epoch 24/50\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.7286 - loss: 0.8970 - val_accuracy: 0.6717 - val_loss: 1.0530\n",
            "Epoch 25/50\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7258 - loss: 0.9002 - val_accuracy: 0.6768 - val_loss: 1.0378\n",
            "Epoch 26/50\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7405 - loss: 0.8627 - val_accuracy: 0.6768 - val_loss: 1.0305\n",
            "Epoch 27/50\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7391 - loss: 0.8738 - val_accuracy: 0.6780 - val_loss: 1.0202\n",
            "Epoch 28/50\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7320 - loss: 0.8660 - val_accuracy: 0.6755 - val_loss: 1.0156\n",
            "Epoch 29/50\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7407 - loss: 0.8441 - val_accuracy: 0.6818 - val_loss: 1.0186\n",
            "Epoch 30/50\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7504 - loss: 0.8254 - val_accuracy: 0.6806 - val_loss: 1.0217\n",
            "Epoch 31/50\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7502 - loss: 0.8256 - val_accuracy: 0.6856 - val_loss: 1.0027\n",
            "Epoch 32/50\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.7492 - loss: 0.8209 - val_accuracy: 0.6768 - val_loss: 1.0120\n",
            "Epoch 33/50\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.7496 - loss: 0.8057 - val_accuracy: 0.6768 - val_loss: 0.9998\n",
            "Epoch 34/50\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.7544 - loss: 0.7991 - val_accuracy: 0.6818 - val_loss: 0.9990\n",
            "Epoch 35/50\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.7547 - loss: 0.7931 - val_accuracy: 0.6818 - val_loss: 1.0057\n",
            "Epoch 36/50\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7519 - loss: 0.7883 - val_accuracy: 0.6869 - val_loss: 0.9978\n",
            "Epoch 37/50\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7657 - loss: 0.7712 - val_accuracy: 0.6856 - val_loss: 0.9958\n",
            "Epoch 38/50\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7619 - loss: 0.7770 - val_accuracy: 0.6919 - val_loss: 0.9882\n",
            "Epoch 39/50\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7661 - loss: 0.7560 - val_accuracy: 0.6919 - val_loss: 1.0006\n",
            "Epoch 40/50\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7646 - loss: 0.7651 - val_accuracy: 0.6932 - val_loss: 0.9846\n",
            "Epoch 41/50\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7708 - loss: 0.7518 - val_accuracy: 0.6944 - val_loss: 0.9883\n",
            "Epoch 42/50\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.7690 - loss: 0.7356 - val_accuracy: 0.6907 - val_loss: 0.9909\n",
            "Epoch 43/50\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - accuracy: 0.7745 - loss: 0.7471 - val_accuracy: 0.6919 - val_loss: 0.9782\n",
            "Epoch 44/50\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.7782 - loss: 0.7299 - val_accuracy: 0.6831 - val_loss: 0.9819\n",
            "Epoch 45/50\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.7724 - loss: 0.7332 - val_accuracy: 0.6869 - val_loss: 0.9768\n",
            "Epoch 46/50\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7728 - loss: 0.7450 - val_accuracy: 0.6894 - val_loss: 0.9825\n",
            "Epoch 47/50\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7846 - loss: 0.7194 - val_accuracy: 0.6894 - val_loss: 0.9829\n",
            "Epoch 48/50\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7857 - loss: 0.7084 - val_accuracy: 0.6957 - val_loss: 0.9827\n",
            "Epoch 49/50\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7782 - loss: 0.7051 - val_accuracy: 0.6894 - val_loss: 0.9822\n",
            "Epoch 50/50\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7807 - loss: 0.7147 - val_accuracy: 0.7008 - val_loss: 0.9765\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x78e6bcae7890>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "clear_session()\n",
        "model.fit(X_train, y_train,\n",
        "                    epochs=50,\n",
        "                    verbose=True,\n",
        "                    validation_data=(X_val, y_val),\n",
        "                    batch_size=64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igrb6ICToUEU",
        "outputId": "1efef46a-83c9-4dc3-8fef-a8c584d65e8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy: 0.7890\n",
            "Testing Accuracy:  0.7089\n"
          ]
        }
      ],
      "source": [
        "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xECmPpLcNHHJ"
      },
      "source": [
        "# Convolutional Neural Networks (CNN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjHqYuzMv10D"
      },
      "source": [
        "Convolutional neural networks or also called convnets.\n",
        "\n",
        "They have revolutionized image classification and computer vision by being able to extract features from images and using them in neural networks. The properties that made them useful in image processing makes them also handy for sequence processing. You can imagine a CNN as a specialized neural network that is able to detect specific patterns.\n",
        "\n",
        "If it is just another neural network, what differentiates it from what you have previously learned?\n",
        "\n",
        "A CNN has hidden layers which are called convolutional layers. When you think of images, a computer has to deal with a two dimensional matrix of numbers and therefore you need some way to detect features in this matrix. These convolutional layers are able to detect edges, corners and other kinds of textures which makes them such a special tool. The convolutional layer consists of multiple filters which are slid across the image and are able to detect specific features.\n",
        "\n",
        "This is the very core of the technique, the mathematical process of convolution. With each convolutional layer the network is able to detect more complex patterns.\n",
        "\n",
        "When you are working with sequential data, like text, you work with one dimensional convolutions, but the idea and the application stays the same. You still want to pick up on patterns in the sequence which become more complex with each added convolutional layer.\n",
        "\n",
        "Now let’s have a look how you can use this network in Keras. Keras offers again various Convolutional layers which you can use for this task. The layer you’ll need is the Conv1D layer. This layer has again various parameters to choose from. The ones you are interested in for now are the number of filters, the kernel size, and the activation function. You can add this layer in between the Embedding layer and the GlobalMaxPool1D layer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "id": "Aepj-wXlv3qG",
        "outputId": "3c81ddf0-ee2b-4e61-973c-1e24228d7447"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling1d        │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling1d        │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "model = Sequential()\n",
        "model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen))\n",
        "model.add(layers.Conv1D(512, 5, activation='relu'))\n",
        "model.add(layers.GlobalAveragePooling1D())\n",
        "model.add(layers.Dense(512, activation='relu'))\n",
        "model.add(layers.Dense(nc, activation='softmax'))\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOVQHSwVwDx1",
        "outputId": "6605f1da-3033-4456-8623-d49b5ed898d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 1s/step - accuracy: 0.2607 - loss: 2.2907 - val_accuracy: 0.5657 - val_loss: 1.3348\n",
            "Epoch 2/5\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 1s/step - accuracy: 0.6963 - loss: 0.9552 - val_accuracy: 0.7437 - val_loss: 0.8363\n",
            "Epoch 3/5\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 1s/step - accuracy: 0.9086 - loss: 0.3269 - val_accuracy: 0.7374 - val_loss: 0.8992\n",
            "Epoch 4/5\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 1s/step - accuracy: 0.9710 - loss: 0.1073 - val_accuracy: 0.7626 - val_loss: 0.9747\n",
            "Epoch 5/5\n",
            "\u001b[1m106/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m7s\u001b[0m 1s/step - accuracy: 0.9937 - loss: 0.0271"
          ]
        }
      ],
      "source": [
        "clear_session()\n",
        "model.fit(X_train, y_train,\n",
        "                    epochs=5,\n",
        "                    verbose=True,\n",
        "                    validation_data=(X_val, y_val),\n",
        "                    batch_size=64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MCJM58jEXOyS"
      },
      "outputs": [],
      "source": [
        "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "loss, accuracy = model.evaluate(X_val, y_val, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 1\n",
        "Try to use a Neural Network to classify the Haspeede and HODI datasets."
      ],
      "metadata": {
        "id": "ifsv4qBW1HQW"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}