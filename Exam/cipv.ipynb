{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fe24978",
   "metadata": {},
   "source": [
    "Import of necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26d9b913",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score,precision_score, recall_score, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score,train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "import time\n",
    "import re\n",
    "from transformers import RobertaTokenizer, RobertaForTokenClassification, pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import warnings\n",
    "import sklearn\n",
    "warnings.filterwarnings('ignore')\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f22776e",
   "metadata": {},
   "source": [
    "Dataset clean (only once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "062757f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset size: 1475\n",
      "üßπ Cleaning and normalizing conversations...\n",
      "Rows after format validation: 857\n",
      "Rows removed due to invalid format: 618\n",
      "Rows after conversation cleaning: 857\n",
      "Rows removed due to cleaning issues: 0\n",
      "\n",
      "üìã Examples of conversation normalization:\n",
      "\n",
      "Example 1:\n",
      "Original: 1. \"Hai assolutamente ragione. Sei cos√¨ unico e speciale. Nessuno √® mai stato come te. √à incredibile...\n",
      "Cleaned:  \"Hai assolutamente ragione. Sei cos√¨ unico e speciale. Nessuno √® mai stato come te. √à incredibile co...\n",
      "\n",
      "Example 2:\n",
      "Original: Maurella                Ho notato che ultimamente non mi dedichi pi√π tempo. Hai qualcosa contro di m...\n",
      "Cleaned:  \"Sono arrabbiata perch√© ti sei ubriacato e mi hai insultata davanti a tutti. Sembri completamente in...\n",
      "\n",
      "üéØ Final cleaned dataset: 857 conversations\n",
      "üíæ Cleaned dataset saved as 'filtered_dataset.csv'\n",
      "\n",
      "üìä Dataset composition:\n",
      "   - Toxic conversations: 593\n",
      "   - Non-toxic conversations: 264\n",
      "   - Total conversations: 857\n"
     ]
    }
   ],
   "source": [
    "dataset= pd.read_csv(\"explaination_toxic_conversation_most_toxic_sentences.csv\",header=0, index_col=0, encoding='utf-8')\n",
    "dataset = dataset[dataset['explaination'].str.contains(\"La frase pi√π tossica √®\", na=False)]\n",
    "# Add a new column 'toxic' with value 1 for all rows\n",
    "dataset['toxic'] = 1\n",
    "\n",
    "non_toxic_dataset= pd.read_csv(\"generated_dataset.csv\",header=0, index_col=0, encoding='utf-8')\n",
    "# Concatenate the two datasets\n",
    "dataset = pd.concat([dataset, non_toxic_dataset], ignore_index=False)\n",
    "\n",
    "def validate_conversation_format(conversation):\n",
    "    \"\"\"\n",
    "    Validates the format of a conversation string.\n",
    "    A valid conversation should:\n",
    "    - Be a non-empty string\n",
    "    - Contain at least two messages, each enclosed in double quotes\n",
    "    \"\"\"\n",
    "    if pd.isna(conversation) or not isinstance(conversation, str):\n",
    "        return False\n",
    "    \n",
    "    # Find all messages enclosed in double quotes\n",
    "    message_pattern = r'\"([^\"]*)\"'\n",
    "    messages = re.findall(message_pattern, conversation)\n",
    "    \n",
    "    # Verify that there are at least two messages\n",
    "    if len(messages) < 2:\n",
    "        return False\n",
    "    \n",
    "    # Check that each message is non-empty after stripping whitespace\n",
    "    if any(not msg.strip() for msg in messages):\n",
    "        return False\n",
    "    \n",
    "    # Check that quotes are even numbered\n",
    "    quote_count = conversation.count('\"')\n",
    "    if quote_count % 2 != 0:\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def clean_and_reconstruct_conversation(conversation):\n",
    "    \"\"\"\n",
    "    Extract messages from conversation and reconstruct it cleanly with only alternating quoted messages.\n",
    "    \"\"\"\n",
    "    if pd.isna(conversation) or not isinstance(conversation, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Extract all messages enclosed in double quotes\n",
    "    message_pattern = r'\"([^\"]*)\"'\n",
    "    messages = re.findall(message_pattern, conversation)\n",
    "    \n",
    "    if len(messages) < 2:\n",
    "        return \"\"\n",
    "    \n",
    "    # Clean each message and filter out empty ones\n",
    "    clean_messages = []\n",
    "    for msg in messages:\n",
    "        cleaned_msg = msg.strip()\n",
    "        if cleaned_msg:  # Only keep non-empty messages\n",
    "            clean_messages.append(f'\"{cleaned_msg}\"')\n",
    "    \n",
    "    # Reconstruct conversation with clean alternating messages\n",
    "    return \" \".join(clean_messages)\n",
    "\n",
    "print(f\"Original dataset size: {len(dataset)}\")\n",
    "\n",
    "# Step 1: Initial validation and cleaning\n",
    "print(\"üßπ Cleaning and normalizing conversations...\")\n",
    "dataset = dataset.dropna(subset=['conversation'])\n",
    "valid_mask = dataset['conversation'].apply(validate_conversation_format)\n",
    "dataset_valid = dataset[valid_mask].copy()\n",
    "\n",
    "print(f\"Rows after format validation: {len(dataset_valid)}\")\n",
    "print(f\"Rows removed due to invalid format: {len(dataset) - len(dataset_valid)}\")\n",
    "\n",
    "# Step 2: Clean and reconstruct conversations\n",
    "dataset_valid['conversation_clean'] = dataset_valid['conversation'].apply(clean_and_reconstruct_conversation)\n",
    "\n",
    "# Remove rows where conversation cleaning failed\n",
    "dataset_clean = dataset_valid[dataset_valid['conversation_clean'] != \"\"].copy()\n",
    "print(f\"Rows after conversation cleaning: {len(dataset_clean)}\")\n",
    "print(f\"Rows removed due to cleaning issues: {len(dataset_valid) - len(dataset_clean)}\")\n",
    "\n",
    "# Replace original conversation with clean version\n",
    "dataset_clean['conversation'] = dataset_clean['conversation_clean']\n",
    "dataset_clean = dataset_clean.drop('conversation_clean', axis=1)\n",
    "\n",
    "# Step 3: Show examples of cleaning\n",
    "print(f\"\\nüìã Examples of conversation normalization:\")\n",
    "for i in range(min(2, len(dataset_clean))):\n",
    "    original = dataset.iloc[i]['conversation'] if i < len(dataset) else \"N/A\"\n",
    "    clean = dataset_clean.iloc[i]['conversation']\n",
    "    \n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Original: {original[:100]}...\")\n",
    "    print(f\"Cleaned:  {clean[:100]}...\")\n",
    "\n",
    "# Update dataset\n",
    "dataset = dataset_clean\n",
    "print(f\"\\nüéØ Final cleaned dataset: {len(dataset)} conversations\")\n",
    "\n",
    "# Save the cleaned dataset\n",
    "dataset.to_csv(\"filtered_dataset.csv\", index=False, encoding='utf-8')\n",
    "print(f\"üíæ Cleaned dataset saved as 'filtered_dataset.csv'\")\n",
    "\n",
    "# Final statistics\n",
    "toxic_count = dataset['toxic'].sum() if 'toxic' in dataset.columns else 0\n",
    "non_toxic_count = len(dataset) - toxic_count\n",
    "print(f\"\\nüìä Dataset composition:\")\n",
    "print(f\"   - Toxic conversations: {toxic_count}\")\n",
    "print(f\"   - Non-toxic conversations: {non_toxic_count}\")\n",
    "print(f\"   - Total conversations: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fee99af",
   "metadata": {},
   "source": [
    "# BINARY CLASSIFICATION : Toxic or not #\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3c6b89",
   "metadata": {},
   "source": [
    "# First Attempt: Pre-processing\n",
    "\n",
    "In this section, we perform binary classification to determine whether a conversation is toxic or not. some pre-processing steps are applied. We apply TF-IDF vectorization directly to the conversation text and train a Logistic Regression model with hyperparameter tuning through nested cross-validation.\n",
    "\n",
    "The model evaluates the following metrics:\n",
    "- Accuracy\n",
    "- F1 Score\n",
    "- Precision\n",
    "- Recall\n",
    "\n",
    "The confusion matrix visualizes the model's performance in distinguishing between toxic and non-toxic conversations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e945b3e",
   "metadata": {},
   "source": [
    "### Let's apply a pre-processing pipeline for text ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a090de89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the Italian language model for spaCy\n",
    "# If you haven't installed the Italian model yet, you can do so with:\n",
    "# !python -m spacy download it_core_news_sm\n",
    "nlp = spacy.load('it_core_news_sm')\n",
    "\n",
    "def preprocess_italian_text(text):\n",
    "    \"\"\"Italian text preprocessing function that tokenizes, lemmatizes, and removes stopwords\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert text to string and lowercase\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # Process the text with spaCy. Doc is a spaCy object that contains the processed text\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Initialize an empty list to hold processed tokens\n",
    "    processed_tokens = []\n",
    "    for token in doc:\n",
    "        # Check if the token is not a stop word, punctuation, space, or a number\n",
    "        if (not token.is_stop and \n",
    "            not token.is_punct and \n",
    "            not token.is_space and \n",
    "            not token.like_num):\n",
    "            # Append the lemmatized token to the processed tokens list\n",
    "            processed_tokens.append(token.lemma_)\n",
    "    \n",
    "    return \" \".join(processed_tokens)\n",
    "\n",
    "dataset=pd.read_csv(\"filtered_dataset.csv\",header=0, encoding='utf-8')\n",
    "#set seed for reproducibility\n",
    "sklearn.utils.check_random_state(seed)\n",
    "#shuffle the dataset\n",
    "dataset = dataset.sample(frac=1, random_state=seed)\n",
    "X = dataset['conversation']\n",
    "y = dataset['toxic']\n",
    "\n",
    "start_time = time.time()\n",
    "# Apply an italian tokenizer, lemmatizer, and stop words\n",
    "X_preprocessed = X.apply(preprocess_italian_text)\n",
    "\n",
    "#Let's apply the TF-IDF vectorizer to the conversation data\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_vectorized = vectorizer.fit_transform(X_preprocessed)\n",
    "#Now we can split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_vectorized, y, test_size=0.3, random_state=seed)\n",
    "\n",
    "print(f\"Dataset shape: {X_vectorized.shape}\")\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "\n",
    "#Let's search for the best hyperparameters for the Logistic Regression model using a Nested Cross-Validation approach\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10],\n",
    "    'penalty': ['l2', 'l1'],\n",
    "    'max_iter': [100, 200, 500],\n",
    "}\n",
    "\n",
    "inner_cv = 5  # 5-fold cross-validation for hyperparameter tuning\n",
    "outer_cv = 5  # 5-fold cross-validation for model evaluation\n",
    "\n",
    "# Create the model\n",
    "logistic_model = LogisticRegression(random_state=seed, solver='liblinear')\n",
    "\n",
    "# Create GridSearchCV object for inner loop\n",
    "grid_search = GridSearchCV(logistic_model, param_grid, cv=inner_cv, scoring='accuracy', n_jobs=-1, verbose=0)\n",
    "\n",
    "# Outer loop: cross-validation for unbiased performance estimation\n",
    "nested_scores = cross_val_score(grid_search, X_train, y_train, cv=outer_cv, scoring='f1', verbose=1)\n",
    "\n",
    "# Fit the grid search on the entire training set to get the best parameters\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(f\"\\nBest parameters found: {grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation F1 score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Now we can train the final model with the best parameters found\n",
    "final_model = LogisticRegression(**grid_search.best_params_, random_state=seed, solver='liblinear')\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = final_model.predict(X_test)\n",
    "y_pred_proba = final_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "test_f1 = f1_score(y_test, y_pred)\n",
    "test_precision = precision_score(y_test, y_pred)\n",
    "test_recall = recall_score(y_test, y_pred)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "end_time = time.time()\n",
    "print(f\"Time taken for training and evaluation:: {end_time - start_time:.2f} seconds\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Test F1-Score: {test_f1:.4f}\")\n",
    "print(f\"Test Precision: {test_precision:.4f}\")\n",
    "print(f\"Test Recall: {test_recall:.4f}\")\n",
    "\n",
    "# Create a confusion matrix with labels\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Not Toxic', 'Toxic'], \n",
    "            yticklabels=['Not Toxic', 'Toxic'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix - With Text Preprocessing')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26caef5b",
   "metadata": {},
   "source": [
    "## What if we don't apply any pre-processing? ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4165260",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=pd.read_csv(\"filtered_dataset.csv\",header=0, encoding='utf-8')\n",
    "#set seed for reproducibility\n",
    "sklearn.utils.check_random_state(seed)\n",
    "#shuffle the dataset\n",
    "dataset = dataset.sample(frac=1, random_state=seed)\n",
    "#take 70% of the dataset for training and 30% for testing\n",
    "#X contains the conversation, y contains the toxic label\n",
    "X = dataset['conversation']\n",
    "y = dataset['toxic']\n",
    "\n",
    "init_time = time.time()\n",
    "#Let's apply the TF-IDF vectorizer to the conversation data\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_vectorized = vectorizer.fit_transform(X)\n",
    "#Now we can split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_vectorized, y, test_size=0.3, random_state=seed)\n",
    "\n",
    "print(f\"Dataset shape: {X_vectorized.shape}\")\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "\n",
    "#Let's search for the best hyperparameters for the Logistic Regression model using a Nested Cross-Validation approach\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10],\n",
    "    'penalty': ['l2', 'l1'],\n",
    "    'max_iter': [100, 200, 500],\n",
    "}\n",
    "\n",
    "\n",
    "inner_cv = 5  # 5-fold cross-validation for hyperparameter tuning\n",
    "outer_cv = 5  # 5-fold cross-validation for model evaluation\n",
    "\n",
    "# Create the model\n",
    "logistic_model = LogisticRegression(random_state=seed, solver='liblinear')\n",
    "\n",
    "# Create GridSearchCV object for inner loop\n",
    "grid_search = GridSearchCV(logistic_model, param_grid, cv=inner_cv, scoring='accuracy', n_jobs=-1, verbose=0)\n",
    "\n",
    "\n",
    "\n",
    "# Outer loop: cross-validation for unbiased performance estimation\n",
    "nested_scores = cross_val_score(grid_search, X_train, y_train, cv=outer_cv, scoring='f1', verbose=1)\n",
    "\n",
    "\n",
    "\n",
    "# Fit the grid search on the entire training set to get the best parameters\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(f\"\\nBest parameters found: {grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation F1 score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Now we can train the final model with the best parameters found\n",
    "final_model = LogisticRegression(**grid_search.best_params_, random_state=seed, solver='liblinear')\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = final_model.predict(X_test)\n",
    "y_pred_proba = final_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "test_f1 = f1_score(y_test, y_pred)\n",
    "test_precision = precision_score(y_test, y_pred)\n",
    "test_recall = recall_score(y_test, y_pred)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "final_time = time.time()\n",
    "print(f\"Time taken for training and evaluation: {final_time - init_time:.2f} seconds\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Test F1-Score: {test_f1:.4f}\")\n",
    "print(f\"Test Precision: {test_precision:.4f}\")\n",
    "print(f\"Test Recall: {test_recall:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create a confusion matrix with labels\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Not Toxic', 'Toxic'], \n",
    "            yticklabels=['Not Toxic', 'Toxic'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60ea95b",
   "metadata": {},
   "source": [
    "## Conclusion ##\n",
    "\n",
    "In this analysis, we performed binary classification to detect toxic conversations using two approaches: one without text preprocessing and one with Italian language-specific preprocessing (tokenization, lemmatization, and stopword removal).\n",
    "\n",
    "**Key Findings:**\n",
    "\n",
    "Both models achieved remarkably similar performance metrics:\n",
    "\n",
    "- **Without preprocessing**: Accuracy: 99%, F1-Score: 99%, Precision: 99%, Recall: 100% (Training time: ~2.35 seconds)\n",
    "- **With preprocessing**: Accuracy: 100%, F1-Score: 100%, Precision: 100%, Recall: 100% (Training time: ~44.39 seconds)\n",
    "\n",
    "**Performance Analysis:**\n",
    "\n",
    "The results demonstrate that both approaches yield virtually identical classification performance, with perfect scores across all metrics. However, there is a significant difference in computational efficiency:\n",
    "\n",
    "- The preprocessing pipeline requires approximately **20 times more computational time** than the raw text approach\n",
    "- Both models achieve perfect classification on the test set, suggesting that the dataset may be well-separated or that the features are highly discriminative\n",
    "\n",
    "**Practical Implications:**\n",
    "\n",
    "Given these results, the text preprocessing step appears to be unnecessary for this particular dataset and classification task. The raw text approach provides:\n",
    "- Equivalent predictive performance\n",
    "- Substantial computational savings\n",
    "- Reduced implementation complexity\n",
    "- Lower resource requirements\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a028cda0",
   "metadata": {},
   "source": [
    "# Conversation TAGGING with RoBERTa #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc160ce",
   "metadata": {},
   "source": [
    "### Let's load and clean our dataset ###\n",
    "\n",
    "Only needed once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b58a8716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the dataset: 1885\n",
      "üßπ Initial dataset cleaning...\n",
      "Original rows: 1885\n",
      "Rows after initial cleaning: 1454\n",
      "Rows removed: 431\n",
      "\n",
      "üîç Checking personality couple mapping...\n",
      "‚ùå Found 3 unmapped personality couples:\n",
      "   - Triste e Consolatore\n",
      "   - Ottimista e Realista\n",
      "   - Entusiasta e Sostenitore\n",
      "These will be excluded from the final dataset!\n",
      "\n",
      "üìä Personality couple distribution:\n",
      "   - Triste e Consolatore: 658 conversations\n",
      "   - Ottimista e Realista: 197 conversations\n",
      "   - Geloso-Ossessivo e Sottomessa: 86 conversations\n",
      "   - Manipolatore e Dipendente emotiva: 73 conversations\n",
      "   - Sadico-Crudele e Masochista: 69 conversations\n",
      "   - Perfezionista Critico e Insicura Cronica: 68 conversations\n",
      "   - Vittimista e Croccerossina: 64 conversations\n",
      "   - Narcisista e Succube: 57 conversations\n",
      "   - Persona violenta e Succube: 50 conversations\n",
      "   - Psicopatico e Adulatrice: 45 conversations\n",
      "   - Controllore e Isolata: 44 conversations\n",
      "   - Dominante e Schiavo emotivo: 37 conversations\n",
      "   - Entusiasta e Sostenitore: 6 conversations\n",
      "\n",
      "üîß Cleaning and reconstructing conversations...\n",
      "Rows after conversation cleaning: 1454\n",
      "Rows removed due to conversation issues: 0\n",
      "\n",
      "üè∑Ô∏è Adding personality tokens...\n",
      "Rows after filtering failed tagging: 593\n",
      "Rows removed due to tagging failures: 861\n",
      "\n",
      "‚úÖ Example of cleaned and tagged conversation:\n",
      "Couple: Psicopatico e Adulatrice\n",
      "Clean conversation: \"Hai assolutamente ragione. Sei cos√¨ unico e speciale. Nessuno √® mai stato come te. √à incredibile come tu riesca sempre ad ottenere ci√≤ che vuoi.\" \"S√¨...\n",
      "Tagged conversation: [START_PSICOPATICO] Hai assolutamente ragione. Sei cos√¨ unico e speciale. Nessuno √® mai stato come te. √à incredibile come tu riesca sempre ad ottenere ci√≤ che vuoi. [END_PSICOPATICO] [START_ADULATRICE...\n",
      "\n",
      "üîç Token verification for first conversation:\n",
      "Tokens found: ['[START_PSICOPATICO]', '[END_PSICOPATICO]', '[START_ADULATRICE]', '[END_ADULATRICE]', '[START_PSICOPATICO]', '[END_PSICOPATICO]']...\n",
      "\n",
      "üéØ- Total conversations: 593\n",
      "üíæ Dataset saved as 'dataset_clean_with_personality_tags.csv'\n"
     ]
    }
   ],
   "source": [
    "dataset=pd.read_csv(\"filtered_dataset.csv\",header=0, index_col=-1, encoding='utf-8')\n",
    "print(\"Number of rows in the dataset:\", len(dataset))\n",
    "\n",
    "def validate_conversation_format(conversation):\n",
    "    \"\"\"\n",
    "    Validates the format of a conversation string.\n",
    "    A valid conversation should:\n",
    "    - Be a non-empty string\n",
    "    - Contain at least two messages, each enclosed in double quotes\n",
    "    \"\"\"\n",
    "    if pd.isna(conversation) or not isinstance(conversation, str):\n",
    "        return False\n",
    "    \n",
    "    # Find all messages enclosed in double quotes\n",
    "    message_pattern = r'\"([^\"]*)\"'\n",
    "    messages = re.findall(message_pattern, conversation)\n",
    "    \n",
    "    # Verify that there are at least two messages\n",
    "    if len(messages) < 2:\n",
    "        return False\n",
    "    \n",
    "    # Check that each message is non-empty after stripping whitespace\n",
    "    if any(not msg.strip() for msg in messages):\n",
    "        return False\n",
    "    \n",
    "    # Check that quotes are even numbered\n",
    "    quote_count = conversation.count('\"')\n",
    "    if quote_count % 2 != 0:\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def extract_personality_types(person_couple):\n",
    "    \"\"\"\n",
    "    Extract the personality types from the given couple string.\n",
    "    \"\"\"\n",
    "    personality_mapping = {\n",
    "        'Psicopatico e Adulatrice': ('Psicopatico', 'Adulatrice'),\n",
    "        'Manipolatore e Dipendente emotiva': ('Manipolatore', 'Dipendente emotiva'),\n",
    "        'Persona violenta e Succube': ('Persona violenta', 'Succube'),\n",
    "        'Narcisista e Succube': ('Narcisista', 'Succube'),\n",
    "        'Sadico-Crudele e Masochista': ('Sadico-Crudele', 'Masochista'),\n",
    "        'Perfezionista Critico e Insicura Cronica': ('Perfezionista Critico', 'Insicura Cronica'),\n",
    "        'Vittimista e Croccerossina': ('Vittimista', 'Croccerossina'),\n",
    "        'Dominante e Schiavo emotivo': ('Dominante', 'Schiavo emotivo'),\n",
    "        'Geloso-Ossessivo e Sottomessa': ('Geloso-Ossessivo', 'Sottomessa'),\n",
    "        'Controllore e Isolata': ('Controllore', 'Isolata')\n",
    "    }\n",
    "    \n",
    "    return personality_mapping.get(person_couple, (None, None))\n",
    "\n",
    "def create_personality_tokens(personality_type):\n",
    "    \"\"\"\n",
    "    Create start and end tokens for a given personality type.\n",
    "    The tokens are formatted as [START_PERSONALITY] and [END_PERSONALITY].\n",
    "    \"\"\"\n",
    "    # Normalize personality name to create valid tokens\n",
    "    clean_personality = personality_type.replace(' ', '_').replace('-', '_').upper()\n",
    "    start_token = f\"[START_{clean_personality}]\"\n",
    "    end_token = f\"[END_{clean_personality}]\"\n",
    "    return start_token, end_token\n",
    "\n",
    "def clean_and_reconstruct_conversation(conversation):\n",
    "    \"\"\"\n",
    "    Extract messages from conversation and reconstruct it cleanly with only alternating quoted messages.\n",
    "    \"\"\"\n",
    "    if pd.isna(conversation) or not isinstance(conversation, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Extract all messages enclosed in double quotes\n",
    "    message_pattern = r'\"([^\"]*)\"'\n",
    "    messages = re.findall(message_pattern, conversation)\n",
    "    \n",
    "    if len(messages) < 2:\n",
    "        return \"\"\n",
    "    \n",
    "    # Clean each message and filter out empty ones\n",
    "    clean_messages = []\n",
    "    for msg in messages:\n",
    "        cleaned_msg = msg.strip()\n",
    "        if cleaned_msg:  # Only keep non-empty messages\n",
    "            clean_messages.append(f'\"{cleaned_msg}\"')\n",
    "    \n",
    "    # Reconstruct conversation with clean alternating messages\n",
    "    return \" \".join(clean_messages)\n",
    "\n",
    "def tag_conversation_with_personalities(conversation, person_couple):\n",
    "    \"\"\"\n",
    "    Apply personality tagging to a conversation based on the given person_couple.\n",
    "    Returns None if tagging fails to ensure data consistency.\n",
    "    \"\"\"\n",
    "    # Extract personality types\n",
    "    personality1, personality2 = extract_personality_types(person_couple)\n",
    "    \n",
    "    if personality1 is None or personality2 is None:\n",
    "        return None  # Return None instead of original conversation\n",
    "    \n",
    "    # Create tokens for both personalities\n",
    "    start_token1, end_token1 = create_personality_tokens(personality1)\n",
    "    start_token2, end_token2 = create_personality_tokens(personality2)\n",
    "    \n",
    "    # Extract all messages from conversation\n",
    "    message_pattern = r'\"([^\"]*)\"'\n",
    "    messages = re.findall(message_pattern, conversation)\n",
    "    \n",
    "    if len(messages) < 2:\n",
    "        return None  # Return None if not enough messages\n",
    "    \n",
    "    # Build tagged conversation\n",
    "    tagged_messages = []\n",
    "    \n",
    "    for i, message in enumerate(messages):\n",
    "        message = message.strip()\n",
    "        \n",
    "        # Alternate between personalities (0,2,4... = personality1; 1,3,5... = personality2)\n",
    "        if i % 2 == 0:\n",
    "            # Message from first personality\n",
    "            tagged_message = f\"{start_token1} {message} {end_token1}\"\n",
    "        else:\n",
    "            # Message from second personality\n",
    "            tagged_message = f\"{start_token2} {message} {end_token2}\"\n",
    "        \n",
    "        tagged_messages.append(tagged_message)\n",
    "    \n",
    "    return \" \".join(tagged_messages)\n",
    "\n",
    "# Step 1: Initial dataset cleaning\n",
    "print(\"üßπ Initial dataset cleaning...\")\n",
    "dataset = dataset.dropna(subset=['conversation'])\n",
    "valid_mask = dataset['conversation'].apply(validate_conversation_format)\n",
    "dataset_cleaned = dataset[valid_mask].copy()\n",
    "\n",
    "print(f\"Original rows: {len(dataset)}\")\n",
    "print(f\"Rows after initial cleaning: {len(dataset_cleaned)}\")\n",
    "print(f\"Rows removed: {len(dataset) - len(dataset_cleaned)}\")\n",
    "\n",
    "# Step 2: Check personality couple mapping\n",
    "print(f\"\\nüîç Checking personality couple mapping...\")\n",
    "unique_couples = dataset_cleaned['person_couple'].unique()\n",
    "unmapped_couples = []\n",
    "\n",
    "for couple in unique_couples:\n",
    "    personality1, personality2 = extract_personality_types(couple)\n",
    "    if personality1 is None or personality2 is None:\n",
    "        unmapped_couples.append(couple)\n",
    "\n",
    "if unmapped_couples:\n",
    "    print(f\"‚ùå Found {len(unmapped_couples)} unmapped personality couples:\")\n",
    "    for couple in unmapped_couples:\n",
    "        print(f\"   - {couple}\")\n",
    "    print(\"These will be excluded from the final dataset!\")\n",
    "else:\n",
    "    print(f\"‚úÖ All {len(unique_couples)} personality couples are properly mapped\")\n",
    "\n",
    "# Show distribution of personality couples\n",
    "print(f\"\\nüìä Personality couple distribution:\")\n",
    "couple_counts = dataset_cleaned['person_couple'].value_counts()\n",
    "for couple, count in couple_counts.items():\n",
    "    print(f\"   - {couple}: {count} conversations\")\n",
    "\n",
    "# Step 3: Clean and reconstruct conversations\n",
    "print(f\"\\nüîß Cleaning and reconstructing conversations...\")\n",
    "dataset_cleaned['conversation_clean'] = dataset_cleaned['conversation'].apply(clean_and_reconstruct_conversation)\n",
    "\n",
    "# Remove rows where conversation cleaning failed\n",
    "before_conv_cleaning = len(dataset_cleaned)\n",
    "dataset_cleaned = dataset_cleaned[dataset_cleaned['conversation_clean'] != \"\"].copy()\n",
    "print(f\"Rows after conversation cleaning: {len(dataset_cleaned)}\")\n",
    "print(f\"Rows removed due to conversation issues: {before_conv_cleaning - len(dataset_cleaned)}\")\n",
    "\n",
    "# Replace original conversation with clean version\n",
    "dataset_cleaned['conversation'] = dataset_cleaned['conversation_clean']\n",
    "dataset_cleaned = dataset_cleaned.drop('conversation_clean', axis=1)\n",
    "\n",
    "# Step 4: Apply personality tagging\n",
    "print(f\"\\nüè∑Ô∏è Adding personality tokens...\")\n",
    "dataset_cleaned['tagged_conversation'] = dataset_cleaned.apply(\n",
    "    lambda row: tag_conversation_with_personalities(row['conversation'], row['person_couple']), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Step 5: Filter out failed tagging\n",
    "before_tagging_filter = len(dataset_cleaned)\n",
    "dataset_final = dataset_cleaned[dataset_cleaned['tagged_conversation'].notna()].copy()\n",
    "print(f\"Rows after filtering failed tagging: {len(dataset_final)}\")\n",
    "print(f\"Rows removed due to tagging failures: {before_tagging_filter - len(dataset_final)}\")\n",
    "\n",
    "# Step 6: Quality check and examples\n",
    "if len(dataset_final) > 0:\n",
    "    print(f\"\\n‚úÖ Example of cleaned and tagged conversation:\")\n",
    "    sample_row = dataset_final.iloc[0]\n",
    "    print(f\"Couple: {sample_row['person_couple']}\")\n",
    "    print(f\"Clean conversation: {sample_row['conversation'][:150]}...\")\n",
    "    print(f\"Tagged conversation: {sample_row['tagged_conversation'][:200]}...\")\n",
    "    \n",
    "    # Show token verification\n",
    "    print(f\"\\nüîç Token verification for first conversation:\")\n",
    "    tokens_found = re.findall(r'\\[START_[A-Z_]+\\]|\\[END_[A-Z_]+\\]', sample_row['tagged_conversation'])\n",
    "    print(f\"Tokens found: {tokens_found[:6]}...\")  # Show first 6 tokens\n",
    "else:\n",
    "    print(\"‚ùå No valid conversations after processing!\")\n",
    "\n",
    "# Step 7: Final dataset preparation\n",
    "dataset = dataset_final\n",
    "print(f\"\\nüéØ- Total conversations: {len(dataset)}\")\n",
    "\n",
    "\n",
    "# Keep only the required columns\n",
    "dataset = dataset[['conversation', 'tagged_conversation']]\n",
    "\n",
    "# Step 8: Save dataset\n",
    "dataset.to_csv(\"dataset_clean_with_personality_tags.csv\", index=False, encoding='utf-8')\n",
    "print(f\"üíæ Dataset saved as 'dataset_clean_with_personality_tags.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0b1dec",
   "metadata": {},
   "source": [
    "# Now let's use RoBERTa #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c530ab95",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m dataset=\u001b[43mpd\u001b[49m.read_csv(\u001b[33m\"\u001b[39m\u001b[33mdataset_clean_with_personality_tags.csv\u001b[39m\u001b[33m\"\u001b[39m,header=\u001b[32m0\u001b[39m, encoding=\u001b[33m'\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mextract_personality_labels_from_tagged\u001b[39m(tagged_conversation):\n\u001b[32m      4\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Extract personality labels and corresponding text from tagged conversation\"\"\"\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Load dataset with personality tags\n",
    "dataset = pd.read_csv(\"dataset_clean_with_personality_tags.csv\", header=0, encoding='utf-8')\n",
    "\n",
    "def extract_all_personalities_from_conversation(tagged_conversation):\n",
    "    \"\"\"Extract all personalities present in a tagged conversation\"\"\"\n",
    "    if pd.isna(tagged_conversation):\n",
    "        return []\n",
    "    \n",
    "    pattern = r'\\[START_([A-Z_]+)\\]'\n",
    "    matches = re.findall(pattern, tagged_conversation)\n",
    "    \n",
    "    personalities = []\n",
    "    for personality in matches:\n",
    "        clean_personality = personality.replace('_', ' ').title()\n",
    "        personalities.append(clean_personality)\n",
    "    \n",
    "    return list(set(personalities))  # Remove duplicates\n",
    "\n",
    "def prepare_multi_label_dataset(dataset):\n",
    "    \"\"\"Prepare dataset for multi-label classification\"\"\"\n",
    "    data = []\n",
    "    \n",
    "    for idx, row in dataset.iterrows():\n",
    "        conversation = row['conversation']\n",
    "        personalities = extract_all_personalities_from_conversation(row['tagged_conversation'])\n",
    "        \n",
    "        if conversation and personalities:\n",
    "            data.append({\n",
    "                'conversation': conversation,\n",
    "                'personalities': personalities\n",
    "            })\n",
    "    \n",
    "    return data\n",
    "\n",
    "def predict_conversation_personalities(conversation, classifier):\n",
    "    \"\"\"Predict all personalities in a conversation using zero-shot classification\"\"\"\n",
    "    candidate_labels = [\n",
    "        \"Psicopatico\", \"Adulatrice\", \"Manipolatore\", \"Dipendente Emotiva\",\n",
    "        \"Persona Violenta\", \"Succube\", \"Narcisista\", \"Perfezionista Critico\", \n",
    "        \"Insicura Cronica\", \"Vittimista\", \"Croccerossina\", \"Dominante\",\n",
    "        \"Schiavo Emotivo\", \"Geloso Ossessivo\", \"Sottomessa\", \"Controllore\", \"Isolata\"\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        result = classifier(conversation, candidate_labels, multi_label=True)\n",
    "        \n",
    "        # Filter predictions with confidence > threshold\n",
    "        threshold = 0.3\n",
    "        predicted_personalities = [\n",
    "            label for label, score in zip(result['labels'], result['scores']) \n",
    "            if score > threshold\n",
    "        ]\n",
    "        \n",
    "        return predicted_personalities\n",
    "    except Exception as e:\n",
    "        return []\n",
    "\n",
    "def calculate_multi_label_metrics(y_true, y_pred):\n",
    "    \"\"\"Calculate metrics for multi-label classification\"\"\"\n",
    "    from sklearn.preprocessing import MultiLabelBinarizer\n",
    "    \n",
    "    # Convert to binary format\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    \n",
    "    # Fit on all possible labels\n",
    "    all_labels = set()\n",
    "    for labels in y_true + y_pred:\n",
    "        all_labels.update(labels)\n",
    "    \n",
    "    mlb.fit([list(all_labels)])\n",
    "    \n",
    "    y_true_bin = mlb.transform(y_true)\n",
    "    y_pred_bin = mlb.transform(y_pred)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_true_bin, y_pred_bin)\n",
    "    f1_micro = f1_score(y_true_bin, y_pred_bin, average='micro', zero_division=0)\n",
    "    f1_macro = f1_score(y_true_bin, y_pred_bin, average='macro', zero_division=0)\n",
    "    precision_micro = precision_score(y_true_bin, y_pred_bin, average='micro', zero_division=0)\n",
    "    recall_micro = recall_score(y_true_bin, y_pred_bin, average='micro', zero_division=0)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1_micro': f1_micro,\n",
    "        'f1_macro': f1_macro,\n",
    "        'precision_micro': precision_micro,\n",
    "        'recall_micro': recall_micro\n",
    "    }\n",
    "\n",
    "# Prepare the dataset\n",
    "print(\"üîÑ Preparing multi-label dataset...\")\n",
    "data = prepare_multi_label_dataset(dataset)\n",
    "\n",
    "# Split into train and test\n",
    "train_data, test_data = train_test_split(data, test_size=0.3, random_state=seed)\n",
    "\n",
    "print(f\"üìä Dataset split:\")\n",
    "print(f\"   Training samples: {len(train_data)}\")\n",
    "print(f\"   Test samples: {len(test_data)}\")\n",
    "\n",
    "# Initialize classifier\n",
    "print(\"ü§ñ Initializing RoBERTa classifier...\")\n",
    "classifier = pipeline(\"zero-shot-classification\", \n",
    "                     model=\"facebook/bart-large-mnli\",\n",
    "                     tokenizer=\"facebook/bart-large-mnli\")\n",
    "\n",
    "# Test on subset for performance\n",
    "test_subset = test_data[:30]  # Test on 30 samples\n",
    "\n",
    "print(\"üöÄ Running predictions on test set...\")\n",
    "start_time = time.time()\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for i, sample in enumerate(test_subset):\n",
    "    conversation = sample['conversation']\n",
    "    true_personalities = sample['personalities']\n",
    "    \n",
    "    predicted_personalities = predict_conversation_personalities(conversation, classifier)\n",
    "    \n",
    "    y_true.append(true_personalities)\n",
    "    y_pred.append(predicted_personalities)\n",
    "    \n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(f\"   Processed {i+1}/{len(test_subset)} samples\")\n",
    "\n",
    "prediction_time = time.time() - start_time\n",
    "\n",
    "# Calculate metrics\n",
    "metrics = calculate_multi_label_metrics(y_true, y_pred)\n",
    "\n",
    "# Calculate baseline (always predict most frequent personality)\n",
    "all_personalities = []\n",
    "for labels in y_true:\n",
    "    all_personalities.extend(labels)\n",
    "\n",
    "from collections import Counter\n",
    "most_frequent = Counter(all_personalities).most_common(1)[0][0]\n",
    "baseline_pred = [[most_frequent] for _ in y_true]\n",
    "baseline_metrics = calculate_multi_label_metrics(y_true, baseline_pred)\n",
    "\n",
    "print(f\"\\nüìà ROBERTA MULTI-LABEL PERFORMANCE:\")\n",
    "print(f\"   üéØ Accuracy: {metrics['accuracy']:.4f}\")\n",
    "print(f\"   üìä F1-Score (Micro): {metrics['f1_micro']:.4f}\")\n",
    "print(f\"   üìä F1-Score (Macro): {metrics['f1_macro']:.4f}\")\n",
    "print(f\"   üìä Precision (Micro): {metrics['precision_micro']:.4f}\")\n",
    "print(f\"   üìä Recall (Micro): {metrics['recall_micro']:.4f}\")\n",
    "print(f\"   ‚è±Ô∏è Time: {prediction_time:.2f}s\")\n",
    "print(f\"   üìà Baseline F1-Micro: {baseline_metrics['f1_micro']:.4f}\")\n",
    "print(f\"   üìà Improvement: {(metrics['f1_micro'] - baseline_metrics['f1_micro']):.4f}\")\n",
    "\n",
    "# Show some examples\n",
    "print(f\"\\nüîç Example predictions:\")\n",
    "for i in range(min(3, len(y_true))):\n",
    "    true_set = set(y_true[i])\n",
    "    pred_set = set(y_pred[i])\n",
    "    \n",
    "    intersection = true_set.intersection(pred_set)\n",
    "    \n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"   True: {sorted(y_true[i])}\")\n",
    "    print(f\"   Predicted: {sorted(y_pred[i])}\")\n",
    "    print(f\"   Correct: {len(intersection)}/{len(true_set)}\")\n",
    "\n",
    "# Visualize personality distribution\n",
    "from collections import Counter\n",
    "true_counts = Counter()\n",
    "pred_counts = Counter()\n",
    "\n",
    "for labels in y_true:\n",
    "    true_counts.update(labels)\n",
    "for labels in y_pred:\n",
    "    pred_counts.update(labels)\n",
    "\n",
    "personalities = sorted(list(set(list(true_counts.keys()) + list(pred_counts.keys()))))\n",
    "true_vals = [true_counts.get(p, 0) for p in personalities]\n",
    "pred_vals = [pred_counts.get(p, 0) for p in personalities]\n",
    "\n",
    "x = range(len(personalities))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 8))\n",
    "ax.bar([i - width/2 for i in x], true_vals, width, label='True', alpha=0.8, color='skyblue')\n",
    "ax.bar([i + width/2 for i in x], pred_vals, width, label='Predicted', alpha=0.8, color='orange')\n",
    "\n",
    "ax.set_xlabel('Personality Types')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Personality Distribution: True vs Predicted')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(personalities, rotation=45, ha='right')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
